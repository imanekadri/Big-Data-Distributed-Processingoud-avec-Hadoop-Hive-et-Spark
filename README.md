# Big Data & Cloud Computing Project
## Hadoop, Hive, Spark & MinIO using Docker

---

## ğŸ“Œ Introduction
Avec lâ€™essor du cloud computing et lâ€™augmentation massive des volumes de donnÃ©es, les technologies Big Data sont devenues indispensables pour le stockage et le traitement distribuÃ© de lâ€™information.  

Ce projet a pour objectif de mettre en Å“uvre une architecture Big Data complÃ¨te en utilisant des solutions open source telles que **Hadoop, Hive, Apache Spark et MinIO**, dÃ©ployÃ©es Ã  lâ€™aide de **Docker**.

Le projet couvre lâ€™ensemble du cycle Big Data :
- Stockage distribuÃ© (HDFS et MinIO)
- Traitement MapReduce
- Interrogation SQL avec Hive
- Analyse avancÃ©e avec Apache Spark
- Automatisation dâ€™un pipeline Big Data

---

## ğŸ§± Architecture du projet

Les services suivants sont dÃ©ployÃ©s sous forme de conteneurs Docker :

- **Hadoop HDFS**
  - NameNode
  - DataNode
- **Hive**
  - Hive Server
  - Metastore
- **Apache Spark**
- **MinIO** (stockage objet compatible Amazon S3)

Cette architecture simule un environnement cloud Big Data rÃ©el sur une machine locale.

---

## ğŸ› ï¸ Technologies utilisÃ©es

- Docker & Docker Compose  
- Hadoop (HDFS, MapReduce)  
- Apache Hive  
- Apache Spark (PySpark)  
- MinIO (S3 compatible)  
- Python  

---

## ğŸ“‚ Structure du projet


MY_PROJECT_BIGDATA/
â”‚

â”œâ”€â”€ docker-compose.yml

â”œâ”€â”€ datasets/

â”‚ â”œâ”€â”€ sales.csv

â”‚ â”œâ”€â”€ products.csv

â”‚ â”œâ”€â”€ web_logs.csv

â”‚
â”œâ”€â”€ scripts/

â”‚ â”œâ”€â”€ mapper_wordcount.py

â”‚ â”œâ”€â”€ reducer_wordcount.py

â”‚ â”œâ”€â”€ sales_mapper.py

â”‚ â”œâ”€â”€ sales_reducer.py

â”‚ â”œâ”€â”€ spark_sales_analysis.py

â”‚ â”œâ”€â”€ web_logs_analysis.py

â”‚ â”œâ”€â”€ pipeline_minio.py

â”‚
â””â”€â”€ README.md


---

## ğŸŒ Interfaces Web

- **HDFS NameNode** : http://localhost:9870  
- **Apache Spark UI** : http://localhost:8080  
- **MinIO Console** : http://localhost:9001  

---

## ğŸš€ Lancement de lâ€™environnement

DÃ©marrer tous les services Docker :

```bash
docker compose up -d
